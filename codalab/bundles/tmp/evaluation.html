<h1>MetaDL Evaluation</h1>
<p>
    
    For both Feedback Phase and Public Phase, the performance of a meta-learning 
    algorithm is measured 
    through the evaluation of 600 episodes at <strong>meta-test</strong> time. 
    The participant needs to implement a <code>MyMetaLearner</code> class that can meta-fit
    a meta-train set and produce a <code>Learner</code> object, which in turn can fit any 
    support set (a.k.a training set), generated from a meta-test set, and 
    produce a <code>Predictor</code>. The accuracy of these predictors on 
    each query set (or test set) is then averaged to produce a final score.
    In Feedback Phase, this score is used to form a leaderboard. In Final Phase,
    this score is used as the criterion for deciding winners (and a leadberboard
    will also be released).
</br>
</br>
    One important aspect of the challenge is that submissions must produce
    a <code>Learner</code> within 2 hours of compute power. The GPU available
    is a <strong>Tesla M60</strong>.
</p>
<h3>Episodes at meta-test time</h3>
<p>
    We use the 5-way 1-shot few-shot learning setting.</br>
</br>
    <strong>Support set:</strong> 5 classes and 1 example per class 
    (labelled examples) <br /><strong>Query set:</strong> 5 classes and a 
    <strong>varying number</strong> of examples per class (unlabelled examples)
</p>
