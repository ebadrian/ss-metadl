{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:2px;border-color:#84C7F7\">\n",
    "<center><h1> Meta-learning competition </h1></center>\n",
    "<center><h2>  Few-shot learning </h2></center>\n",
    "<hr style=\"border-width:2px;border-color:#84C7F7\">\n",
    "\n",
    "Make sure you have the **meta_dataset** and **metadl** packages installed in your kernel environment. If you ran the <code>quick_start.sh</code> script, make sure you activated the **metadl** conda environment before launching the jupyter notebook. Here is the link of the [CodaLab competition](https://competitions.codalab.org/competitions/26212?secret_key=a50a8a46-e33a-497c-9121-d56a0b576c07) where you can submit your code and check the leaderboard.\n",
    "\n",
    "\n",
    "<u>**Outline**</u> : \n",
    "* **I - Data exploration** : We define the few-shot learning setup and explore how the data is formatted\n",
    "* **II - Submission details** : We present how a submission should be organized\n",
    "* **III - Test and submission** : We present how to test a potential submission and also how to zip your scripts to submit your code on CodaLab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import gin\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from meta_dataset.data import config\n",
    "from meta_dataset.data import pipeline\n",
    "from meta_dataset.data import learning_spec\n",
    "from meta_dataset.data import dataset_spec as dataset_spec_lib\n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "def plot_episode(support_images, support_class_ids, query_images,\n",
    "                 query_class_ids, size_multiplier=1, max_imgs_per_col=10,\n",
    "                 max_imgs_per_row=10):\n",
    "    \"\"\"Plots the content of an episode. Episodes are composed of a support set \n",
    "    (training set) and a query set (test set). The different numbers of examples in\n",
    "    each set will be detailled in the starting kit.\n",
    "    Args:\n",
    "        - support_images : tuple, (Batch_size_support, Height, Width, Channels)\n",
    "        - support_class_ids : tuple, (Batch_size_support, N_class)\n",
    "        - query_images : tuple, (Batch_size_query, Height, Width, Channels)\n",
    "        - size_multiplier : dilate or shrink the size of displayed images\n",
    "        - max_imgs_per_col : Integer, Number of images in a column\n",
    "        - max_imgs_per_row : Integer, Number of images in a row\n",
    "    \"\"\"\n",
    "    \n",
    "    for name, images, class_ids in zip(('Support', 'Query'),\n",
    "                                     (support_images, query_images),\n",
    "                                     (support_class_ids, query_class_ids)):\n",
    "        n_samples_per_class = Counter(class_ids)\n",
    "        n_samples_per_class = {k: min(v, max_imgs_per_col)\n",
    "                               for k, v in n_samples_per_class.items()}\n",
    "        id_plot_index_map = {k: i for i, k\n",
    "                             in enumerate(n_samples_per_class.keys())}\n",
    "        num_classes = min(max_imgs_per_row, len(n_samples_per_class.keys()))\n",
    "        max_n_sample = max(n_samples_per_class.values())\n",
    "        figwidth = max_n_sample\n",
    "        figheight = num_classes\n",
    "        if name == 'Support':\n",
    "            print('#Classes: %d' % len(n_samples_per_class.keys()))\n",
    "        figsize = (figheight * size_multiplier, figwidth * size_multiplier)\n",
    "        fig, axarr = plt.subplots(\n",
    "            figwidth, figheight, figsize=figsize)\n",
    "        fig.suptitle('%s Set' % name, size='15')\n",
    "        fig.tight_layout(pad=3, w_pad=0.1, h_pad=0.1)\n",
    "        reverse_id_map = {v: k for k, v in id_plot_index_map.items()}\n",
    "        for i, ax in enumerate(axarr.flat):\n",
    "            ax.patch.set_alpha(0)\n",
    "            # Print the class ids, this is needed since, we want to set the x axis\n",
    "            # even there is no picture.\n",
    "            ax.set(xlabel=reverse_id_map[i % figheight], xticks=[], yticks=[])\n",
    "            ax.label_outer()\n",
    "        for image, class_id in zip(images, class_ids):\n",
    "            # First decrement by one to find last spot for the class id.\n",
    "            n_samples_per_class[class_id] -= 1\n",
    "            # If class column is filled or not represented: pass.\n",
    "            if (n_samples_per_class[class_id] < 0 or\n",
    "              id_plot_index_map[class_id] >= max_imgs_per_row):\n",
    "                continue\n",
    "            # If width or height is 1, then axarr is a vector.\n",
    "            if axarr.ndim == 1:\n",
    "                ax = axarr[n_samples_per_class[class_id]\n",
    "                           if figheight == 1 else id_plot_index_map[class_id]]\n",
    "            else:\n",
    "                ax = axarr[n_samples_per_class[class_id], id_plot_index_map[class_id]]\n",
    "            ax.imshow(image / 2 + 0.5)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def iterate_dataset(dataset, n):\n",
    "    \"\"\" Iterates over an episode generator represented by dataset.\n",
    "    It yields n episodes. An episode is a tuple containing images from \n",
    "    the support (train set) and query set (test set). A full episode description\n",
    "    is available in the starting kit.\n",
    "    \"\"\"\n",
    "    if not tf.executing_eagerly():\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "        with tf.Session() as sess:\n",
    "            for idx in range(n):\n",
    "                yield idx, sess.run(next_element)\n",
    "    else:\n",
    "        for idx, episode in enumerate(dataset):\n",
    "            if idx == n:\n",
    "                break\n",
    "            yield idx, episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data exploration\n",
    " The goal of this section is to familiarize participants with the data format used in the challenge.\n",
    "\n",
    "Few-shot learning procedures aim to produce a Learner that is able to quickly adapt to unseen tasks with a few examples. \n",
    "In the standard Machine Learning setting, we usually split the data in train/test sets, these datasets then contain **examples** assumed to be generated from the same distribution. In few-shot learning, we have the same idea but with one additional level of abstraction : we have a meta-train and meta-test split (optionnally a meta-validation split as well). Indeed, meta-train and meta-test dataset are assumed to have **classes** generated from the same **task distribution**. For instance, we consider the Omniglot dataset during the public phase of the challenge. Omniglot is composed of 1623 classes that makes it interesting for meta-learning problems. We seperate these classes in 3 splits, a meta-train, meta-validation and meta-test sets. \n",
    "* **Meta-training** : with data sampled from the meta-train pool, we could meta-train a MetaLearner, i.e. try to learn the best approach to tackle different tasks.\n",
    "* **Meta-validation** : with data sampled from the meta-validation pool, we could adjust the meta-learner's hyper-parameters without worrying about any data leakage.\n",
    "* **Meta-testing** : with data sampled from the meta-test pool, we evaluate the Learner produced by the meta-learning procedure to quickly adapt to new unseen tasks. In order to measure the performance of such behavior, we define what we call **episodes**. These are small tasks, i.e. with a few training examples of unseen classes.\n",
    "\n",
    "Let's formalize some of the ideas exposed above.\n",
    "\n",
    "## Definitions\n",
    " Previously we mentionned the possibility of generating data from a specific split pool. There are 2 different ways to generate data in this challenge. We can either generate data in the form of **episodes** or **batches**. Let's first describe these 2 methods : \n",
    "\n",
    "An **episode**, which represents a **task**, is described as follows : \n",
    "$$ \\mathcal{T} = \\{ \\mathcal{D}_{train}, \\mathcal{D}_{test}\\}$$\n",
    "where $\\mathcal{D}_{train} = \\{x_{i}, y_{i}\\}_{i \\in \\mathcal{I}_{train}}$ is the training set of the task, often called **support set**. $\\mathcal{D}_{test} = \\{x_{i}, y_{i}\\}_{i \\in \\mathcal{I}_{test}}$ is the test set of the task, often called **query set**. Note that $\\mathcal{I}_{train}$ and $\\mathcal{I}_{test}$ are indices of the train and test set examples respectively.\n",
    "\n",
    "A **batch** is a collection of examples sampled from a split pool but **without enforcing a configuration**. For instance, let's say we want to generate data in batch mode from the meta-train pool. We can specify the batch size which is the number of examples to be sampled from the pool. We would directly sample examples from the pool without sampling **classes** as it is the case for episodes. More importantly, there would be no aforementionned $\\mathcal{D}_{test}$ unlike the episodic setting. In order to better visualize the difference with the episode setting, we provide a figure below to illustrate these 2 methods.\n",
    "\n",
    "![Challenge API](FSL_setting.png)\n",
    "\n",
    "\n",
    "## The few-shot learning problem\n",
    "\n",
    "The few-shot learning problems are often referred as N-way K-shots problem. This name refers to episodes configuration at **meta-test time**. The number of **ways** N denotes the number of classes in an episode that represents an image classification problem. The number of **shots** K denotes the number of examples per class in the **support set**. In our case, we focus on the **5-way 1-shot** setting. In other words, episodes at meta-test time represent image classification problems with exactly 5 classes, and the **support set** contains 1 labelled example per class. More formally, $|\\mathcal{I}_{train} | = 5$. \n",
    "Let's summarize the different parts of the meta-learning procedure.\n",
    "\n",
    "* At **meta-train** time : This is the part you have control on. You can choose to generate data from the meta-train split in the form of **episodes** or **batches**.\n",
    "* At **meta-test** time : We always evaluate your few-shot learning algorithm using the same setting, we generate new unseen tasks from the meta-test pool in the form of episodes. Actually these episodes have a fixed configuration, the 5-way 1-shot setting. It essentially means that when you receive a new unseen task, the support set (i.e. train set) will be composed of 5 examples, 1 for each class represented. The query set (i.e. test set) is composed of multiple unlabelled examples corresponding to theses classes. We control the number of examples in the query set, and it depends on the challenge phase. For instance, for the Omniglot dataset, the episodes' query sets at meta-test time are composed of 19 examples per class (95 examples in total).\n",
    "\n",
    "In this challenge, the episodes are generated **on the fly** from our datasets. Also, it is worth mentioning that the episodes and batches are coming from **generators**, meaning that there are virtually infinite. \n",
    " \n",
    "The number of examples in the **query set** usually depends on the number of examples for each class available in a dataset. In the public dataset (Omniglot), each class has 20 examples and we thus decide to set $|\\mathcal{I}_{test}| = |N| * 19$ = 95. A visual example using the Omniglot dataset using this setting is displayed in the figure below. Note that this setting is an example, one could change the way the data is received at meta-train time. For example we could change the number of classes in an episode at meta-train time as in the prototypical networks algorithm. That is, you creates episodes containing 60 classes at meta-train time and evaluate the meta-learning algorithm performance with episodes containing 5 classes at meta-test time. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Note** : Make sure you have downloaded the public data under <code>omniglot/</code> directory in the root directory of this project, i.e. **../metadl**.\n",
    "Let's see how it looks like in practice : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadl.data.dataset import DataGenerator\n",
    "\n",
    "config_episode = [28, 5, 1, 19] # [img_size, N_ways, K_shot, nbr_query_ex]\n",
    "meta_train_dir = '../../omniglot/meta_train' # Path to Public data\n",
    "\n",
    "\n",
    "# The DataGenerator initialization creates 2 generators as attributes :\n",
    "# Meta-train data generator : meta_train_pipeline\n",
    "# Meta-valid data generator : meta_valid_pipeline\n",
    "data_generator = DataGenerator(path_to_records=meta_train_dir,\n",
    "                                batch_config=None,\n",
    "                                episode_config=config_episode,\n",
    "                                valid_episode_config=config_episode,\n",
    "                                pool='train',\n",
    "                                mode='episode')\n",
    "meta_train_generator = data_generator.meta_train_pipeline\n",
    "meta_valid_generator = data_generator.meta_valid_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell, we created a <code>DataGenerator</code> object. You receive data during meta-training through this object. Notice that you can specify the configuration of meta-train and meta-valid episodes, but you could switch to <code>mode='batch'</code> if you think it would improve your meta-algorithm performance. We are going to visualize data generated as **episodes** and **batches** in the next code cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES=2\n",
    "\n",
    "dataset_spec = dataset_spec_lib.load_dataset_spec(meta_train_dir)\n",
    "all_dataset_specs = [dataset_spec]\n",
    "\n",
    "for idx, (episode, source_id) in iterate_dataset(meta_train_generator, N_EPISODES):\n",
    "    print('Episode id: %d from source %s' % (idx, all_dataset_specs[source_id].name))\n",
    "    episode = [a.numpy() for a in episode]\n",
    "    plot_episode(support_images=episode[0], support_class_ids=episode[2],\n",
    "               query_images=episode[3], query_class_ids=episode[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figures above, you can observe the composition of an episode : A **support set** (train) and a **query set** (test). In the next cell, we present some useful caracteristics of an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of the tuple describing an episode : {} \\n'\\\n",
    "      .format(len(episode)))\n",
    "print('#'*70)\n",
    "print('\\nThe episode tuple is organized the following way : \\n \\n ' + \n",
    "     '[Support_images, Support_labels, Support_original_labels,' + \n",
    "      'Query_images, Query_labels, Query_original_labels] \\n')\n",
    "print('#'*70)\n",
    "print('\\nThe support set images are of the following shape : {} \\n'\\\n",
    "      .format(episode[0].shape))\n",
    "print('The support set labels are : {} and their shape : {} \\n'\\\n",
    "      .format(episode[1], episode[1].shape))\n",
    "\n",
    "print('The support set original labels in the dataset from which' +\n",
    "        ' they are sampled : {} and shape : {}\\n'\\\n",
    "       .format(episode[2], episode[2].shape))\n",
    "print('#'*70)\n",
    "print('\\nThe query set images are of the following shape : {} \\n'\\\n",
    "      .format(episode[3].shape))\n",
    "print('The query set labels shape is : {} \\n'\\\n",
    "      .format(episode[4].shape))\n",
    "\n",
    "print('The query set original labels shape in the dataset from which' +\n",
    "        ' they are sampled is : {} \\n'\\\n",
    "       .format(episode[5].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the **batch** mode. \n",
    "Let's define the batch configuration as \\[28, 30\\] indicating that we'd like to receive data from the meta-train split in batches of 30 images of shape \\[28,28,3\\]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataGenerator initialization creates 2 generators as attributes :\n",
    "# Meta-train data generator : meta_train_pipeline\n",
    "# Meta-valid data generator : meta_valid_pipeline\n",
    "batch_data_generator = DataGenerator(path_to_records=meta_train_dir,\n",
    "                                    batch_config=[28, 30],\n",
    "                                    episode_config=None,\n",
    "                                    valid_episode_config=[28,5,1,19],\n",
    "                                    pool='train',\n",
    "                                    mode='batch')\n",
    "\n",
    "meta_train_generator = batch_data_generator.meta_train_pipeline\n",
    "meta_valid_generator = batch_data_generator.meta_valid_pipeline\n",
    "\n",
    "meta_train_iterator = meta_train_generator.__iter__()\n",
    "((images, labels), _) = next(meta_train_iterator)\n",
    "print(f'Batch images shape : {images.shape}')\n",
    "print(f'Batch labels shape : {labels.shape}')\n",
    "\n",
    "def plot_batch(images, labels, size_multiplier=1):\n",
    "    \"\"\" Plot the images in a batch. Notice that labels,\n",
    "    corresponds to images original class id_s.\n",
    "    Args:\n",
    "        images: tf.Tensor, shape \n",
    "                (batch_size, image_size, image_size, 3)\n",
    "        labels: tf.Tensor, shape (batch_size,)\n",
    "        size_multiplier: Float, defines how big images will\n",
    "            be displayed.\n",
    "    \"\"\"\n",
    "    num_examples = len(labels)\n",
    "    figwidth = np.ceil(np.sqrt(num_examples)).astype('int32')\n",
    "    figheight = num_examples // figwidth\n",
    "    figsize = (figwidth * size_multiplier, (figheight + 2.5) * size_multiplier)\n",
    "    _, axarr = plt.subplots(figwidth, figheight, dpi=300, figsize=figsize)\n",
    "\n",
    "    for i, ax in enumerate(axarr.transpose().ravel()):\n",
    "        # Images are between -1 and 1.\n",
    "        ax.imshow(images[i] / 2 + 0.5)\n",
    "        ax.set(xlabel=str(labels[i].numpy()), xticks=[], yticks=[])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_batch(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the challenge, you don't need to create your generators, you will receive a DataGenerator object (thus already initialized). The way you receive the DataGenerator object will be described in the next section. The default setting is the episodic setting 5-way 1-shot for every meta-split. However, if you do think you could achieve better performance with your own meta-training setting, you can specify it. In order to specify your own setting, you need to write down your settings in a gin file named **config.gin** and put it in your submission folder before zipping it. We will go over the structure of submission folder in the next sections. Here is an example of a config file for the prototypical networks algorithms : \n",
    "\n",
    "**Content of a <code>config.gin</code> file**:\n",
    "```bash\n",
    "DataGenerator.batch_config = None\n",
    "DataGenerator.episode_config = [ 28, 60, 1, 5 ]\n",
    "DataGenerator.valid_episode_config = [ 28, 5, 1, 19 ]\n",
    "DataGenerator.pool = 'train'\n",
    "DataGenerator.mode = 'episode'\n",
    "```\n",
    "First, notice the configuration of episodes coming from the meta-train split, described by **episode_config**. The first value denotes the image size of the received images, here we kept the original value 28. Then you can specify the number of classes in your episodes, here it is set to 60! Then you specify the number of shots **K**, here 1. And finally you can specify the number of query examples per class, here 5. In this example, the meta-validation episodes description, **valid_episode_config**, is set to <code>[28, 5, 1, 19]</code> to match the episode configuration at meta-test time.\n",
    "\n",
    "\n",
    "For clarity here are the configuration descriptions : \n",
    "\n",
    "<code>episode_config = [img_size, num_ways, num_shots_per_class, num_query_per_class]</code>\n",
    "\n",
    "<code>batch_config = [img_size, batch_size]</code>\n",
    "\n",
    "---\n",
    "\n",
    "**Section summary** :\n",
    "\n",
    "* You can choose to generate data from the meta-train split in the form of episodes or batches. Default configurations are episodic but you can change it via a **config.gin** file that you put in your folder submission.\n",
    "* You can choose to have access to episodes coming from the meta-validation split to match the evaluation at meta-test time. However, we do not allow you to generate data from the meta-validation split in batch mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Submission details\n",
    "In this section, we will review the structure of a valid submission. We will see that the data we receive for the few-shot learning algorithm follows the aforementioned structure.\n",
    "\n",
    "\n",
    "\n",
    "The participants would have to submit a zip file containing one or several files. The crucial file to add is <code>model.py</code>. It contains the meta-learning algorithm logic. This file **has** to follow the specific API that we defined for the challenge described in the following figure : \n",
    "\n",
    "![Challenge API](Api.png)\n",
    "\n",
    "The 3 classes with their associated methods that need to be overriden are the following :\n",
    "* **MetaLearner** : The meta-learner contains the meta-algorithm logic. The <code>meta_fit(data_generator)</code> method has to be overriden with your own meta-learning algorithm. It receives a DataGenerator object initialized with default setting or your **config.gin** file.\n",
    "\n",
    "* **Learner** : It encapsulates the logic to learn from a new unseen task. Several methods need to be overriden : \n",
    " * <code>fit(D_train)</code>: Takes a support (train) set as an argument and fit the learner according to this dataset.\n",
    " * <code>save()</code> : You need to implement a way to save your model in a pre-defined directory. \n",
    " * <code>load()</code> : You need to implement a way to load your model from the file you created in <code>save()</code>.\n",
    "* **Predictor** : The predictor contains the logic of your model to make predictions once the learner is fitted. The <code>predict(D_test)</code> encapsulates this step and takes a query (test) set as an argument, i.e. unlabelled examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough a submission example\n",
    "\n",
    "In this sub-section, we present how your code submission folder should look like before zipping it.  \n",
    "\n",
    "**Example of a submission directory**\n",
    "```\n",
    "proto\n",
    "|   metadata  (Mandatory)\n",
    "│   model.py  (Mandatory)\n",
    "│   model.gin (Optional but has to have this name)\n",
    "|   config.gin (Optional but has to have this name)\n",
    "│   helper.py (Optional) \n",
    "│   utils.py  (Optional)\n",
    "│   ...\n",
    "```\n",
    "<code>model.py</code> and <code>metadata</code> are the crucial files to be added. The former contains your few-shot learning algorithm and the latter is just a file for the competition server to work properly, you simply add it to your folder without worrying about it (you can find this file in any given baseline's folder). Other files could be added and it us up to you to organize your code as you'd like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the classes\n",
    "We go through a dummy example to understand how to create a model. In the code cell below, you can find the **zero** baseline. There are 2 important remarks :\n",
    "* First, it is mandatory to **write a file** in the <code>model_dir</code> given as an argument in the <code>save()</code> method. It could be a any file, some metadata that you gathered and/or your serialized neural network, but you need to include one.\n",
    "* Then, one can notice that the shape of the tensor returned by the <code>predict</code> method is (95,5). Indeed, The number of query examples is set to 95 for the episodes generated from the **meta-test** dataset, in the **Omniglot** dataset (i.e. the public dataset). Make sure your own predictions match the shape of the corresponding challenge phase. You can check the outputs shape in the CodaLab competition website. \n",
    "\n",
    "**Note** : You can always test your algorithm with <code>run.py</code> to verify everything is working properly. We explain how to run the script in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadl.api.api import MetaLearner, Learner, Predictor\n",
    "\n",
    "class MyMetaLearner(MetaLearner):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def meta_fit(self, meta_dataset_generator) -> Learner:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            meta_dataset_generator : a DataGenerator object. We can access \n",
    "                the meta-train and meta-validation data via its attributes.\n",
    "                Refer to the metadl/data/dataset.py for more details.\n",
    "        \n",
    "        Returns:\n",
    "            MyLearner object : a Learner that stores the meta-learner's \n",
    "                learning object. (e.g. a neural network trained on meta-train\n",
    "                episodes)\n",
    "        \"\"\"\n",
    "        return MyLearner()\n",
    "\n",
    "class MyLearner(Learner):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, dataset_train) -> Predictor:\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            dataset_train : a tf.data.Dataset object. It is an iterator over \n",
    "                the support examples.\n",
    "        Returns:\n",
    "            ModelPredictor : a Predictor.\n",
    "        \"\"\"\n",
    "        return MyPredictor()\n",
    "\n",
    "    def save(self, model_dir):\n",
    "        \"\"\" Saves the learning object associated to the Learner. It could be \n",
    "        a neural network for example. \n",
    "\n",
    "        Note : It is mandatory to write a file in model_dir. Otherwise, your \n",
    "        code won't be available in the scoring process (and thus it won't be \n",
    "        a valid submission).\n",
    "        \"\"\"\n",
    "        if(os.path.isdir(model_dir) != True):\n",
    "            raise ValueError(('The model directory provided is invalid. Please'\n",
    "                + ' check that its path is valid.'))\n",
    "        \n",
    "        # Save a file for the code submission to work correctly.\n",
    "        with open(os.path.join(model_dir,'dummy_sample.csv'), 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(['Dummy example'])\n",
    "            \n",
    "    def load(self, model_dir):\n",
    "        \"\"\" Loads the learning object associated to the Learner. It should \n",
    "        match the way you saved this object in save().\n",
    "        \"\"\"\n",
    "        if(os.path.isdir(model_dir) != True):\n",
    "            raise ValueError(('The model directory provided is invalid. Please'\n",
    "                + ' check that its path is valid.'))\n",
    "        \n",
    "    \n",
    "class MyPredictor(Predictor):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, dataset_test):\n",
    "        \"\"\" Predicts the label of the examples in the query set which is the \n",
    "        dataset_test in this case. The prototypes are already computed by\n",
    "        the Learner.\n",
    "\n",
    "        Args:\n",
    "            dataset_test : a tf.data.Dataset object. An iterator over the \n",
    "                unlabelled query examples.\n",
    "        Returns: \n",
    "            preds : tensors, shape (num_examples, N_ways). We are using the \n",
    "                Sparse Categorical Accuracy to evaluate the predictions. Valid \n",
    "                tensors can take 2 different forms described below.\n",
    "\n",
    "        Case 1 : The i-th prediction row contains the i-th example logits.\n",
    "        Case 2 : The i-th prediction row contains the i-th example \n",
    "                probabilities.\n",
    "\n",
    "        Since in both cases the SparseCategoricalAccuracy behaves the same way,\n",
    "        i.e. taking the argmax of the row inputs, both forms are valid.\n",
    "        Note : In the challenge N_ways = 5 at meta-test time.\n",
    "        \"\"\"\n",
    "        # mimick the softmax outputs\n",
    "        dummy_pred = tf.constant([[1.0, 0, 0, 0 ,0]], dtype=tf.float32)\n",
    "        dummy_pred = tf.broadcast_to(dummy_pred, (95, 5))\n",
    "        return dummy_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refer to the <code>metadl/baselines/</code> folder if you want to see submission examples. Here are the algorithms provided : \n",
    "* The **dummy zero** baseline  \n",
    "* The **Prototypical Networks** based on  [J. Snell et al. - Prototypical Networks for Few-shot Learning (2017)](https://arxiv.org/pdf/1703.05175)\n",
    "* The **fo-MAML** algorithm based on [C. Finn et al. - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (2017)](https://arxiv.org/pdf/1703.03400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Test and Submission\n",
    "\n",
    "Here we present the <code>run.py</code> script. It is meant to mimick what is happenning on the CodaLab platform, i.e. the competition server. Let's say you worked on an algorithm and you are ready to test it before submitting it. More specifically, it will create your MetaLearner object, run the meta-fit method and evaluate your meta-algorithm on test episodes generated from the meta-test split. You can run the script command with the following arguments :\n",
    "* <code>meta_dataset_dir</code> : The path which contains the **2 meta-datasets**, the meta-train dataset and the meta-test dataset. The <code>quick_start.sh</code> script that you executed or the Docker image, downloaded the public dataset : Omniglot. \n",
    "* <code>code_dir</code> : The path which contains your **algorithm's code** following the format we previously defined. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m metadl.core.run --meta_dataset_dir=../../omniglot --code_dir=../baselines/zero \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a ZIP file ready for submission\n",
    "Here we present how to zip your code to submit it on the CodaLab platform. As an example, we zip the folder <code>metadl/baselines/zero/</code> which corresponds to the dummy baseline which was introduced in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zip_utils import zipdir\n",
    "\n",
    "model_dir = '../baselines/zero/'\n",
    "submission_filename = 'mysubmission.zip'\n",
    "zipdir(submission_filename, model_dir)\n",
    "print('Submit this file :' + submission_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "For clarity, we summarize the steps that you should be aware of while making a submission : \n",
    "* Follow the **MetaLearner**/**Learner**/**Predictor** API to encapsulate your few-shot learning algorithm. Please make sure you name your subclasses as **MyMetaLearner**, **MyLearner** and **MyPredictor** respectively.\n",
    "* Make sure you <u>save</u> at least a file in the given <code>model_dir</code> path. If this is a trained neural network, you need to serialize it in the <code>save()</code> method, and provide code to deserialize it in the <code>load()</code> method. Examples are provided in <code>metadl/baselines/</code>.\n",
    "* In your algorithm folder, make sure you have <code>model.py</code> and <code>metadata</code> with these **exact** names. If you do use gin files, be sure to use the corresponding names as in the baselines, i.e. **model.gin** for your own model parameters and **config.gin** for the data generation configuration (batch vs episodes).\n",
    "\n",
    "--- \n",
    "\n",
    "## Next steps\n",
    "Now you know all the steps required to create a valid code submission.\n",
    "\n",
    "Good luck !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}